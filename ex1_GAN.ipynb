{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Generative Adversarial Network (DCGAN)\n",
    "## Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will guide you through the process of building a Generative Adversarial Network with convolutional layers (DCGAN).\n",
    "\n",
    "Our aim is to generate samples that resemble as much as possible the ones that are given in the training set, by drawing from vectors in a latent space.\n",
    "\n",
    "DCGANs consist of two competitive networks: a Generator and a Discriminator. In order to achieve our goal, we train their parameters in order to minimize two loss functions: $\\mathcal{L_{disc}}$ and $\\mathcal{L_{gen}}$ for the discriminator and the generator respectively\n",
    "\n",
    "In the following notebook we will focus on the application of generative methods in Machine Learning to a proxy dataset for a Monte Carlo simulation output in a High Energy Physics experiment. \n",
    "\n",
    "In such datasets not only the common metrics used to gauge the performances of a GAN network are relevant (such as the Discriminator accuracy or the loss values) but also other statistic information are, for example the distributions of the variables that the dataset embeds. \n",
    "\n",
    "For a comprehensive treatment of DCGANs refer to:\n",
    "1. https://arxiv.org/abs/1406.2661\n",
    "2. https://arxiv.org/abs/1606.03498v1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Important imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow.keras.backend as kb\n",
    "\n",
    "\n",
    "from IPython import display\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get some utils from:\n",
    "!git clone https://github.com/dlanci/UZHMLWorkshop2020-GAN\n",
    "os.chdir('UZHMLWorkshop2020-GAN/')\n",
    "from utils.utils import load_dataset\n",
    "from utils.utils import generate_and_save_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset file\n",
    "!wget -q --show-progress -O batch0.pickle \"https://www.dropbox.com/s/wn8ilvp8k67grz4/batch0.pickle?dl=0\";\n",
    "!wget -q --show-progress -O batch1.pickle \"https://www.dropbox.com/s/5vjme0o8drbi0v4/batch1.pickle?dl=0\";\n",
    "!wget -q --show-progress -O batch2.pickle \"https://www.dropbox.com/s/rz2b8c4911kb4iy/batch2.pickle?dl=0\";\n",
    "!wget -q --show-progress -O batch3.pickle \"https://www.dropbox.com/s/2wa94zzt7wq2002/batch3.pickle?dl=0\";\n",
    "!wget -q --show-progress -O batch4.pickle \"https://www.dropbox.com/s/icntmsval8nync5/batch4.pickle?dl=0\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_, tot_evts = load_dataset(path='.', test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 And explore it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just loaded a python $\\texttt{dict}$, and for this notebook we will use the numpy $\\texttt{ndarray}$ that is accessible at the key: <span style=\"color:red\">'reco_imgs'</span>. Such tensor has dimensions:\n",
    "\n",
    "$$\n",
    "tuple\\_['reco\\_imgs'].shape = [tot\\_evts,\n",
    "                                X\\_pixels, \n",
    "                                Y\\_pixels,\n",
    "                                1]\n",
    "$$\n",
    "\n",
    "It contains a number tot_events of images whose pixels map the energy deposit of a particle in the calorimeter, note that the n_channels for each of these images is 1 as the energy deposited in every pixel is a scalar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LHCb Hadron Calorimeter\n",
    "---------------\n",
    "![Center](./nb_img/calo.jpeg)\n",
    "\n",
    "from https://cds.cern.ch/record/1293073/files/Guz.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_pixels=48   # n of cells in horizontal. direction\n",
    "Y_pixels=48   # n of cells in horizontal. direction\n",
    "\n",
    "\n",
    "#Use fancy indexing to divide the loaded dataset \n",
    "#in train and test subsamples such that the train_set\n",
    "#contains 90% of the original sample and the test_set\n",
    "#the remainder\n",
    "\n",
    "X_train=tuple_['reco_imgs'][0:np.int(tot_evts*0.9)]\n",
    "X_test=tuple_['reco_imgs'][np.int(tot_evts*0.9):tot_evts]\n",
    "\n",
    "\n",
    "\n",
    "# ___________________________________________________________________\n",
    "#now let's normalise the energy deposit per cell so that\n",
    "#it reaches 1 as a maximum value\n",
    "\n",
    "maxval=X_train[np.where(X_train!=0)].max()\n",
    "\n",
    "X_train_norm = (X_train)/(maxval)\n",
    "X_test_norm = (X_test)/(maxval)\n",
    "\n",
    "\n",
    "# ___________________________________________________________________\n",
    "#and let's take a look at one of those images, and\n",
    "#the total energy deposit per image distribution \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X_train_norm[2].reshape(X_pixels,Y_pixels))\n",
    "plt.xlabel('X', fontsize=15)\n",
    "plt.ylabel('Y', fontsize=15)\n",
    "plt.title('A sample image', fontsize=15)\n",
    "plt.colorbar()\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(np.sum(X_train_norm,axis=(1,2,3)),bins=100);\n",
    "plt.xlabel('E (a.u.)', fontsize=15)\n",
    "plt.ylabel('dN/dE', fontsize=15)\n",
    "plt.title('Total energy deposit per image', fontsize=15)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tensorflow Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 2.x handles the data pipe-in, the batching and shuffling of the dataset (such as many other useful utilities such as reshaping and one-hot encoding) through the $\\texttt{Dataset}$ function.\n",
    "\n",
    "(See https://www.tensorflow.org/api_docs/python/tf/data/Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fix some numbers\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_TRAIN_SIZE = np.int(tot_evts*0.9)\n",
    "BUFFER_TEST_SIZE = np.int(tot_evts*0.1)\n",
    "\n",
    "\n",
    "# ___________________________________________________________________\n",
    "# Our dataset tensor is sliced along the first dimension, \n",
    "# (i.e. the # of events dimension) with from_tensor_slices:\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(X_train_norm)\n",
    "test_dataset  = tf.data.Dataset.from_tensor_slices(X_test_norm)\n",
    "\n",
    "print(\"The full train dataset:              \",train_dataset)\n",
    "\n",
    "# ___________________________________________________________________\n",
    "# The sliced tensor is then shuffled, we set the BUFFER_SIZE as \n",
    "# the shape[0] of the X_train, and the option of reshuffling\n",
    "# at every epoch.\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_TRAIN_SIZE, \n",
    "                                      reshuffle_each_iteration=True)\n",
    "test_dataset = test_dataset.shuffle(BUFFER_TEST_SIZE, \n",
    "                                      reshuffle_each_iteration=True)\n",
    "\n",
    "print(\"The shuffled train dataset:          \",train_dataset)\n",
    "\n",
    "# ___________________________________________________________________\n",
    "# We divide our dataset tensor is batches of BATCH_SIZE images, and\n",
    "# drop the remainder of events that exceed the sample batching\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, \n",
    "                                    drop_remainder=True)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, \n",
    "                                    drop_remainder=True)\n",
    "\n",
    "print(\"The batched train dataset:           \",train_dataset)\n",
    "\n",
    "\n",
    "# ___________________________________________________________________\n",
    "# elements in the dataset can stiil be accessed\n",
    "# through an iterator \n",
    "\n",
    "it = iter(train_dataset)\n",
    "img = next(it)\n",
    "plt.imshow(img.numpy()[0].reshape(48,48))\n",
    "plt.xlabel('X', fontsize=15)\n",
    "plt.ylabel('Y', fontsize=15)\n",
    "plt.title('A sample image', fontsize=15)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Building the Generator model\n",
    "\n",
    "Tensorflow is further adopting Keras as their high level API. In this notebook we will make use of the functional API as it is a way to create models that are more flexible than the $\\texttt{tf.keras.Sequential}$ API **[1]** . The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
    "\n",
    "We'll define two <span style=\"color:blue\">functions </span> each one returning a model, one for the generator (G) and one for the discriminator (D), let's start from G:\n",
    "\n",
    "The generator is an upsampling network that maps points in the latent space to higher dimension objects, in our case also to higher rank objects as the output are (48x48) images. The key method for an upsampling network is the deconvolution operation and we will implement it with Conv2DTranspose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Center](./nb_img/GAN/gen.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful references:**\n",
    "\n",
    "* [1] https://www.tensorflow.org/guide/keras/functional\n",
    "\n",
    "* [2] A guide to the arithmetic of convolution\n",
    "https://arxiv.org/pdf/1603.07285v1.pdf\n",
    "\n",
    "* [3] A comprehensive treatment of the Deconvolution operation:\n",
    "https://distill.pub/2016/deconv-checkerboard/\n",
    "\n",
    "* [4] https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model(z_dim):\n",
    "    \n",
    "    n_nodes = 12*12\n",
    "    \n",
    "        \n",
    "    z_input_init = layers.Input((z_dim,)) # As opposed to the Sequential API, the functional API \n",
    "                                          # require the specification of the layer.Input, as an\n",
    "                                          # argument the shape (z_dim,) is the shape of the latent\n",
    "                                          # space input vector\n",
    "    \n",
    "                           #output dim, # no bias b\n",
    "    z_input = layers.Dense(64 * n_nodes, use_bias=False)(z_input_init) # Here we propagate the input\n",
    "    z_input = layers.LeakyReLU(alpha=0.2)(z_input)                     # through dense connected layers    \n",
    "    z_input = layers.Reshape((12, 12, 64))(z_input)                    # and reshape it as a rank 3 tensor\n",
    "        \n",
    "    # note that the 1st dimension (batch_size) is not specified\n",
    "    \n",
    "\n",
    "    hid = layers.Conv2DTranspose(filters=32,                           # number of output channels\n",
    "                                 kernel_size=(4,4),                    # size of the convolution kernel              \n",
    "                                 strides=(2,2),                        \n",
    "                                 padding='same',\n",
    "                                 use_bias=False)(z_input)\n",
    "    #print(hid.shape)\n",
    "\n",
    "    \n",
    "    hid=layers.BatchNormalization()(hid)\n",
    "    hid=layers.LeakyReLU(alpha=0.2)(hid)\n",
    "    \n",
    "    \n",
    "    hid = layers.Conv2DTranspose(filters=16,\n",
    "                                 kernel_size=(5,5), \n",
    "                                 strides=(2,2), \n",
    "                                 padding='same',\n",
    "                                 use_bias=False)(hid)\n",
    "    \n",
    "    #print(hid.shape)\n",
    "    \n",
    "    hid=layers.BatchNormalization()(hid)\n",
    "    hid=layers.LeakyReLU(alpha=0.2)(hid)    \n",
    "    \n",
    "    out = layers.Conv2DTranspose(filters=1,\n",
    "                                 kernel_size=(5,5), \n",
    "                                 strides=(1,1), \n",
    "                                 padding='same',\n",
    "                                 use_bias=False,\n",
    "                                 activation='sigmoid')(hid)\n",
    "\n",
    "    #print(hid.shape)\n",
    "    \n",
    "    model = models.Model(z_input_init, outputs=out)   #specify inputs and outputs of your model\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim=128                                      # dimension of the latent vector for the generator\n",
    "generator = make_generator_model(z_dim=noise_dim)  # let's create the generator and print\n",
    "generator.summary()                                # a useful summary of the created layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Building the Discriminator model\n",
    "\n",
    "\n",
    "The discriminator is an downsampling network that is trained to tell apart true from fake images. The true images are the dataset images while fake images are the ones produced by the discriminator. The key method to downsample and extract important features of the image is the convolution operation:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Center](./nb_img/GAN/disc.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful references**\n",
    "\n",
    "* [1] https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
    "* [2] https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Minibatch discrimination\n",
    "\n",
    "Minibatch discrimination is an important method to avoid the Generator to collapse to a parameter setting where it always emits the same point. Mode collapse of the generator is often connected to the Discriminator returning the same output for similar inputs. In this way all Generator outputs race toward a single point that the Discriminator currently believes is highly realistic.\n",
    "\n",
    "Minibatch discrimination thus helps the Discriminator to telling the outputs of the Generator to become more dissimilar to each other\n",
    "\n",
    "Let $f(x_{i}) \\in R^{A}$ denote the output of some intermediate layer of the discriminator ($i$ runs over the batch size  and $A$ is the features dimension). When multiplied by a tensor $T \\in R^{AxBxC}$ we obtain a matrix $M_{i} \\in R^{BxC}$. We compute the $L_{1}$-distance between rows of the resulting matrix across samples and apply a negative exponential\n",
    "\n",
    "$$\n",
    "o(x_{i}) = \\sum_{j=1}^{n} \\text{exp}( -|| M_{i} - M_{j}||)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The task of the discriminator is thus effectively still to classify single examples as real data or generated data, but it is now able to use the other examples in the minibatch as side information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    \n",
    "\n",
    "    \n",
    "    image_input_layer = layers.Input((X_pixels,Y_pixels,1))                     # Again define the input\n",
    "    \n",
    "    hid = layers.Conv2D(64, kernel_size=4, strides=2, padding='same')(image_input_layer) # Example usage\n",
    "    hid = layers.LeakyReLU(alpha=0.2)(hid)                                               # of a convolutional\n",
    "    hid = layers.Dropout(0.3)(hid)                                                       # layer\n",
    "    \n",
    "    #print(hid.output)\n",
    "    \n",
    "    hid = layers.Conv2D(128, kernel_size=4, strides=2, padding='same')(hid)\n",
    "    hid = layers.LeakyReLU(alpha=0.2)(hid)\n",
    "    hid = layers.Dropout(0.3)(hid)\n",
    "    \n",
    "    #print(hid.output)\n",
    "    \n",
    "    feature = layers.Flatten()(hid)     #Here I call the output \"feature\" for future needs\n",
    "    \n",
    "    hid = layers.Dense(32)(feature)\n",
    "    hid = layers.LeakyReLU(alpha=0.2)(hid)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #MINIBATCH DISCRIMINATION\n",
    "    \n",
    "    #dimensions of minibatch matrix\n",
    "    n_kernels=12\n",
    "    dim_kernel=12\n",
    "    \n",
    "    mb_discr = layers.Dense(n_kernels*dim_kernel)(hid)\n",
    "    mb_discr = layers.Reshape((n_kernels,dim_kernel))(mb_discr)  #Matrix M\n",
    "    \n",
    "    diffs = kb.expand_dims(mb_discr, 3)-kb.expand_dims(kb.permute_dimensions(mb_discr, [1, 2, 0]), 0)\n",
    "    abs_diffs = kb.sum(kb.abs(diffs), axis=2)\n",
    "    \n",
    "    minibatch_features = kb.sum(kb.exp(-abs_diffs),2)\n",
    "    print(minibatch_features.shape)# (None, n_kernels)\n",
    "    hid=layers.Concatenate()([hid, minibatch_features])\n",
    "    \"\"\"\n",
    "    \n",
    "    out = layers.Dense(1)(hid) #note that we don't activate the last layer of the discriminator\n",
    "                               #as the loss function we will use requires the unactivated output\n",
    "    \n",
    "    model = models.Model(inputs=image_input_layer, outputs=[out, feature])\n",
    "    \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tensorflow 2.x: eager execution\n",
    "\n",
    "TensorFlow's allows the user to evaluates operations immediately **[1]** , without building graphs: operations return concrete values instead of constructing a computational graph to run later (as it was in tf 1.X). This makes it easy to get started with TensorFlow and debug models. In our case we can visualize an output of the untrained generator and the corresponding (unactivated) output of the discriminator:\n",
    "\n",
    "**Useful references**\n",
    "\n",
    "* [1] https://www.tensorflow.org/guide/eager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal((1, noise_dim))\n",
    "generated_image = generator(noise, training=False)\n",
    "plt.imshow(generated_image[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision, _ = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Definition of the Losses\n",
    "\n",
    "In the next step we will define the functions to be minimized during the training process. The two models (G,D) participate in a non-cooperative game and try to minimize two different loss functions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Discriminator Loss\n",
    "\n",
    "The Discriminator's task is to correctly tell apart images of the original dataset from images created by the Generator network, in other words we want to minimize the negative cross-entropy cost [1] on the Discriminator's predictions:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{D} = - \\underbrace{\\log(D(x))}_{\\mathcal{L}^{\\text{true images}}_{D}} - \\underbrace{\\log(1-D(G(z)))}_{\\mathcal{L}^{\\text{fake images}}_{D}}\n",
    "$$\n",
    "\n",
    "Where $D(x)$ represents the activated output of the discriminator on a sample from the true images dataset and $D(G(z))$ represents the activated output of the discriminator on a sample produced by the generator.\n",
    "\n",
    "\n",
    "**Note:**  $\\mathcal{L}^{\\text{true images}}_{D}$ is minimum as $D(x)$ approaches 1 and conversely $\\mathcal{L}^{\\text{fake images}}_{D}$ is minimum as $D(G(z))$ approaches 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 One sided label smoothing\n",
    "\n",
    "It was shown in [2] that replacing the targets for a classifier (0,1) with one sided smoothed values (0,0.99) improved convergency of the network, for this reason we set our labels to a smoothed value in the discriminator and generator losses\n",
    "\n",
    "**Useful links**\n",
    "\n",
    "* [1] https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n",
    "* [2] https://arxiv.org/abs/1606.03498v1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output, eps=1e-3):\n",
    "\n",
    "    real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=real_output,\n",
    "                labels=tf.ones_like(real_output))) #one sided label smoothing\n",
    "                #labels=(1-eps)*tf.ones_like(real_output))) #one sided label smoothing\n",
    "    \n",
    "    \n",
    "    fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=fake_output,\n",
    "                labels=tf.zeros_like(fake_output)))\n",
    "    \n",
    "    total_loss = real_loss + fake_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)   #define the minimization algorithm here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Generator Loss\n",
    "\n",
    "Following the same reasoning that we used for the Discriminator we maximise the probability for the Generator to \"fool\" the discriminator and generate samples that are similar to the original dataset by minimizing the loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{G} = - \\underbrace{\\log(D(G(z)))}_{\\mathcal{L}^{\\text{fake images}}_{G}}\n",
    "$$\n",
    "\n",
    "\n",
    "**Note:** The Generator never sees how a sample from the true dataset looks like, he's only trained based on the output of the Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Feature matching\n",
    "\n",
    "Feature matching is a technique adopted to address the instability of GANs by specifying a new loss function for the Generator. Instead of directly minimizing $\\mathcal{L}_{G}$ the new loss function requires the Generator to match the expected value of the features on an intermediate layer of the discriminator (see 3.2)\n",
    "The new loss of the generator becomes then:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{G} = || \\mathbb{E}_{x \\sim p_{data}}f(x) - \\mathbb{E}_{z \\sim p_{z}} f(G(z))||^{2}_{2}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss_GAN(fake_output, eps=1e-3):\n",
    "    gan_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=fake_output,\n",
    "                labels=tf.ones_like(fake_output))) \n",
    "                #labels=(1-eps)*tf.ones_like(fake_output))) #one sided label smoothing\n",
    "    return gan_loss\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "FEATURE MATCHING\n",
    "\n",
    "\n",
    "def generator_loss_FEATURE(true_features, fake_features):\n",
    "    gan_loss= tf.sqrt(tf.reduce_mean(tf.pow(true_features-fake_features,2)))\n",
    "    return gan_loss\n",
    "\n",
    "\"\"\"    \n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)   #define the minimization algorithm here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Set up the training cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Tensorboard for metrics inspection\n",
    "\n",
    "Tensorboard is a useful tool to gather and compare the common metrics used to gauge the performance of our networks, in the next lines we set up the needed objects for it\n",
    "\n",
    "**Useful links**\n",
    "\n",
    "* Tensorboard toolkit https://www.tensorflow.org/tensorboard\n",
    "* Keras metrics module https://www.tensorflow.org/api_docs/python/tf/keras/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our metrics\n",
    "\n",
    "d_train_loss = tf.keras.metrics.Mean('d_train_loss', dtype=tf.float32)\n",
    "g_train_loss = tf.keras.metrics.Mean('g_train_loss', dtype=tf.float32)\n",
    "d_train_accuracy_on_real = tf.keras.metrics.BinaryCrossentropy('train_accuracy_on_real', from_logits=True)\n",
    "d_train_accuracy_on_fake = tf.keras.metrics.BinaryCrossentropy('train_accuracy_on_fake', from_logits=True)\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = './GAN/logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 The train step function\n",
    "\n",
    "In this step we define the function that regulates each training step. If you used tf 1.X in this step you would be defining tf.Sessions and tf.placeholders to build the graph underlying your training cycles.\n",
    "The way you create a graph in tf 2.x is by creating a tf.function [1] or by decorating a Python callable (the train_step() function in our case)\n",
    "\n",
    "**Useful links**\n",
    "\n",
    "* [1] https://www.tensorflow.org/api_docs/python/tf/function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples_to_generate = 16                                   #seed for testing \n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])  #purposes\n",
    "\n",
    "@tf.function #tf.function decorator, this causes the function to be \"compiled\".\n",
    "def train_step(images):\n",
    "    \n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as gen_tape, tf.GradientTape(persistent=True) as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output, real_feature = discriminator(images, training=True)\n",
    "        fake_output, fake_feature = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss_GAN(fake_output)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        ENABLE FEATURE MATCHING\n",
    "        \n",
    "        gen_loss = generator_loss_FEATURE(real_feature, fake_feature)        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "        \n",
    "    for i in range(1): \n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "                                                                       \n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)                                                                       \n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    d_train_loss(disc_loss)\n",
    "    g_train_loss(gen_loss)\n",
    "    d_train_accuracy_on_real(tf.ones_like(real_output), real_output)\n",
    "    d_train_accuracy_on_fake(tf.zeros_like(fake_output), fake_output)\n",
    "    \n",
    "    \n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 The train step function\n",
    "\n",
    "In this step we define the function that regulates each training step. If you used tf 1.X in this step you would be defining tf.Sessions and tf.placeholders to build the graph underlying your training cycles. The way you create a graph in tf 2.x is by creating a tf.function or by decorating a Python callable (the train_step() function in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example snippet on how to save intermediate statuses of the network (checkpoints)\n",
    "\n",
    "checkpoint_dir = './GAN/training_checkpoints' #directory in which the checkpoint will be saved\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def train(dataset, epochs):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"____________\")\n",
    "        print(\"Epoch {}\".format(epoch))        \n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        for image_batch in train_dataset:\n",
    "            \n",
    "            g_loss, d_loss = train_step(image_batch)\n",
    "\n",
    "        \n",
    "        with train_summary_writer.as_default():\n",
    "            \n",
    "            tf.summary.scalar('d_loss', d_train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('g_loss', g_train_loss.result(), step=epoch)            \n",
    "            tf.summary.scalar('d_accuracy_on_fake', d_train_accuracy_on_fake.result(), step=epoch)\n",
    "            tf.summary.scalar('d_accuracy_on_real', d_train_accuracy_on_real.result(), step=epoch)            \n",
    "        \n",
    "        \n",
    "        # Produce images for the GIF as we go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator,\n",
    "                                 epoch + 1,\n",
    "                                 seed, maxval)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Save a checkpoint of the model every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "            \n",
    "\n",
    "        print('Time for epoch {0} is {1} sec'.format(epoch + 1, datetime.datetime.now()-start))\n",
    "        print(\"Done Epoch {} \".format(epoch))\n",
    "        print(\"____________\")\n",
    "    # Generate a batch of images after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    generate_and_save_images(generator,  #function defined in the\n",
    "                               epochs,   #utils.py, plots\n",
    "                               seed,\n",
    "                               maxval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./GAN/logs/gradient_tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Deploy the model to generate a fake MC sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Cope with real life conditions\n",
    "\n",
    "Often we're deploying these toy models in a limited resources environment, and to generate a statistically significant ensemble we cannot feed input latent vectors of arbitrary shape. Thus we have to resort to workarounds like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionary={} #create a python dictionary that will contain the full set\n",
    "\n",
    "num_examples_to_generate=20000 # specify the total size of the set\n",
    "n_samples_per_batch=2000       # specify \n",
    "\n",
    "n_cycles = num_examples_to_generate//n_samples_per_batch  #get the number of cycles as an integer\n",
    "\n",
    "for i in range(n_cycles):\n",
    "    seed = tf.random.normal([n_samples_per_batch, noise_dim]) #create a new seed for every iteration\n",
    "    predictionary[i]=generator(seed, training=False).numpy()  #assign the numpy-ed version of the output\n",
    "                                                              #of the i-th iteration\n",
    "    \n",
    "out=np.concatenate([predictionary[i] for i in range(n_cycles)])  #Create the total set by concatenating\n",
    "                                                                 #the different dict items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Compare the fake E distribution with the true one\n",
    "\n",
    "In scientific applications of DCGANs the standard metrics used in section 5.1 and the by-eye verisimilitude of single samples is not enough, in our case for example the energy distribution of the simulated sample is of key importance for physics applications, let's inspect the performance of this network in picking up the energy distribution of the true sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "vals_gen=plt.hist(np.sum(out,axis=(1,2,3)), range=(0,9), density=True, bins=50, edgecolor='black')[0];\n",
    "plt.subplot(1,2,2)\n",
    "vals_true=plt.hist(np.sum(X_train_norm,axis=(1,2,3)), density=True, range=(0,9),bins=50, edgecolor='black')[0];\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(0,9, step=9/50)\n",
    "diff=plt.bar(idx, height=(vals_true-vals_gen), edgecolor='black',\n",
    "            linewidth=1, color='lightblue',width = .15, align = 'edge')\n",
    "\n",
    "plt.xlabel('E (GeV)')\n",
    "plt.ylabel('dN/dE)')\n",
    "plt.title(\"MC - NN output\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
